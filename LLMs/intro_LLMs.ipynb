{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96641eb-c8b4-4489-8428-b90f15061be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe58396-8d37-46c1-83cf-7bdbf58fa7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP_DIR is set to: /fs/ddn/sdf/group/atlas/d/lapereir/miniconda3/envs/LLMs/lib/python3.9/site-packages\n"
     ]
    }
   ],
   "source": [
    "# torchtext is deprecated, but you can use it for a basic exerice\n",
    "import os\n",
    "import site\n",
    "\n",
    "# Set the SP_DIR environment variable\n",
    "os.environ['SP_DIR'] = site.getsitepackages()[0]\n",
    "\n",
    "# Verify that it has been set correctly\n",
    "print(\"SP_DIR is set to:\", os.environ['SP_DIR'])\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e0b93d-05a1-400c-ac9c-dabb4b02fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test classifier\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from IPython.display import Markdown as md\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "258be3e2-1dca-4b70-94a3-11aa120ccf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /sdf/home/l/lapereir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /sdf/home/l/lapereir/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfdad442-76a0-4e2f-8bda-303b9d73efec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't help the dog. Can't you do it? Don't be afraid if you are.\n",
      "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /sdf/home/l/lapereir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /sdf/home/l/lapereir/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Word Based tokenizers from the Natural Language Toolkit (nlktk) library\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "\n",
    "# word_tokenize from nltk library\n",
    "tokens = word_tokenize(text)\n",
    "print(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efd0358d-cd32-409d-bb4d-d6143876d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n",
      "Since en_core_web_sm is a pre-trained model, it provides additional info:\n",
      "-------------------------\n",
      "Token (text , pos_, dep_)\n",
      "-------------------------\n",
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN nsubj\n",
      "startup VERB ccomp\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# Word Based tokenizer from spaCy (available in multiple languages)' \n",
    "import spacy\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from spacy.lang.en.examples import sentences \n",
    "\n",
    "text = sentences[0] # load example from spaCy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained model packages provided by spaCy for processing English text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Making a list of the tokens and priting the list\n",
    "token_list = [token.text for token in doc]\n",
    "print(doc.text)\n",
    "print(token_list)\n",
    "\n",
    "print(\"Since en_core_web_sm is a pre-trained model, it provides additional info:\")\n",
    "print(\"-------------------------\")\n",
    "print(\"Token (text , pos_, dep_)\")\n",
    "print(\"-------------------------\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8b0d96-4fae-4315-bc48-724a2928f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't help the dog. Can't you do it? Don't be afraid if you are.\n",
      "['i', 'couldn', \"'\", 't', 'help', 'the', 'dog', '.', 'can', \"'\", 't', 'you', 'do', 'it', '?', 'don', \"'\", 't', 'be', 'afraid', 'if', 'you', 'are', '.']\n",
      "['▁I', '▁couldn', \"'\", 't', '▁help', '▁the', '▁dog', '.', '▁Can', \"'\", 't', '▁you', '▁do', '▁it', '?', '▁Don', \"'\", 't', '▁be', '▁afraid', '▁if', '▁you', '▁are', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sub-word based Tokenizers from HuggingFace\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "print(text)\n",
    "\n",
    "#BERT is a WordPiiece tokenizer i.e. initializes its vocabulary to include every character present in the training data and progressively learns a specified number of merge rules. WordPiece doesn't select the most frequent symbol pair but rather the one that maximizes the likelihood of the training data when added to the vocabulary. In essence, WordPiece evaluates what it sacrifices by merging two symbols to ensure it's a worthwhile endeavor.\n",
    "\n",
    "#Now, the WordPiece tokenizer is implemented in BertTokenizer. \n",
    "#Note that BertTokenizer treats composite words as separate tokens.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print( tokenizer.tokenize(text))\n",
    "\n",
    "#XLNetTokenizer uses Unigram and SentencePiece \n",
    "#Unigram is a method for breaking words or text into smaller pieces. It accomplishes this by starting with a large list of possibilities and gradually narrowing it down based on how frequently those pieces appear in the text. This approach aids in efficient text tokenization.\n",
    "#SentencePiece is a tool that takes text, divides it into smaller, more manageable parts, assigns IDs to these segments, and ensures that it does so consistently. Consequently, if you use SentencePiece on the same text repeatedly, you will consistently obtain the same subwords and IDs.\n",
    "#Unigram and SentencePiece work together by implementing Unigram's subword tokenization method within the SentencePiece framework. SentencePiece handles subword segmentation and ID assignment, while Unigram's principles guide the vocabulary reduction process to create a more efficient representation of the text data. This combination is particularly valuable for various NLP tasks in which subword tokenization can enhance the performance of language models.\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30f4856-baf8-42b8-a286-14f01ae27c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['open-r1/OpenR1-Math-220k', 'saiyan-world/Goku-MovieGenBench', 'open-thoughts/OpenThoughts-114k', 'fka/awesome-chatgpt-prompts', 'Anthropic/EconomicIndex', 'AI-MO/NuminaMath-1.5', 'FreedomIntelligence/medical-o1-reasoning-SFT', 'simplescaling/s1K', 'open-r1/OpenR1-Math-Raw', 'GAIR/LIMO', 'HuggingFaceFW/fineweb', 'agentica-org/DeepScaleR-Preview-Dataset', 'zed-industries/zeta', 'ServiceNow-AI/R1-Distill-SFT', 'cognitivecomputations/dolphin-r1', 'CausalLM/Retrieval-SFT-Chat', 'simplescaling/s1K-1.1', 'bespokelabs/Bespoke-Stratos-17k', 'PleIAs/common_corpus', 'cais/hle']\n"
     ]
    }
   ],
   "source": [
    "# Explore datasets available in HuggingFace\n",
    "\n",
    "from huggingface_hub import list_datasets\n",
    "# List all available datasets\n",
    "datasets = list_datasets()\n",
    "# Print some dataset names to see what's available\n",
    "datasets_info = [dataset.id for dataset in datasets]  # Extract dataset identifiers\n",
    "\n",
    "# Print some of the available datasets\n",
    "print(datasets_info[:20])  \n",
    "\n",
    "# see all at https://huggingface.co/datasets?language=language:en&sort=trending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "205916c0-1f1c-4c24-9f20-206c2ba4c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1927248db3045d6824031ee47712a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f139862c39344e1aa0bdcfe1e8cf2301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd807b6d652443b5bf3b685007f319c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8f6eaeb81e42ae8b0a0248f5cfca72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4233cc492a8b4c2f916d335d5b129d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# Use HuggingFace libraries\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the a hugging face dataset\n",
    "dataset = load_dataset('ag_news') # test dataset\n",
    "print(dataset)\n",
    "\n",
    "###########################\n",
    "## Load your own data\n",
    "###########################\n",
    "# Directory containing all text files\n",
    "#path_to_text_files = 'path/to/text/files/'\n",
    "# Load the dataset\n",
    "#dataset = load_dataset('text', data_files={'train': f'{path_to_text_files}*.txt'})\n",
    "\n",
    "# Path to the CSV file\n",
    "#path_to_data = 'path/to/your/dataset.csv'\n",
    "# Load the dataset\n",
    "#dataset = load_dataset('csv', data_files=path_to_data)\n",
    "\n",
    "# Explore the dataset\n",
    "print(dataset['train'][0])  # Assuming it automatically splits or you have defined the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0299e8fa-f4f6-4ce5-af9c-18c48970def1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      " Tokenized text:  ['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'd', '##wind', '##ling', '\\\\', 'band', 'of', 'ultra', '-', 'cy', '##nic', '##s', ',', 'are', 'seeing', 'green', 'again', '.']\n",
      "\n",
      " Label:  2\n"
     ]
    }
   ],
   "source": [
    "# Use HuggingFace libraries\n",
    "\n",
    "# now tokenize the data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #wordpiece\n",
    "#tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased') #unigram+senetencepiece\n",
    "# or simply tokenize\n",
    "\n",
    "data =  dataset['train'][0]\n",
    "\n",
    "text =data['text']\n",
    "print(text)\n",
    "label = data['label']\n",
    "print(\"\\n Tokenized text: \", tokenizer.tokenize(text))\n",
    "print(\"\\n Label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df9719fc-da0f-45e4-8fb2-6c945ba3d9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ecf3c3f5634b93923ba8b238d68588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8f9c363dbc41f396305247b30f6019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2, 'input_ids': [101, 2813, 2358, 1012, 6468, 15020, 2067, 2046, 1996, 2304, 1006, 26665, 1007, 26665, 1011, 2460, 1011, 19041, 1010, 2813, 2395, 1005, 1055, 1040, 11101, 2989, 1032, 2316, 1997, 11087, 1011, 22330, 8713, 2015, 1010, 2024, 3773, 2665, 2153, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #wordpiece\n",
    "#tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased') #unigram+senetencepiece\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])#, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization across the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Look at the first example\n",
    "print(tokenized_datasets['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aee21bae-66f6-4bb1-b4f9-2ae7a10251b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the dataset\n",
    "train_iter = iter(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42a41cfb-405d-4d76-94d0-6abcb8dac163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: ['Spam Slayer: New Tools Fight Phishing Scams Swindlers combine spam with hoax sites to try to rip off your personal data.']\n",
      "Labels: tensor([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you need a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def convert_examples_to_features(example):\n",
    "    # Process the text through a tokenizer if using models, or leave as is for basic handling\n",
    "    return {\n",
    "        'text': example['text'],\n",
    "        'labels': example['label']\n",
    "    }\n",
    "# Create a DataLoader\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# explore dataloader in loop\n",
    "for batch in test_dataloader:\n",
    "    texts = batch['text']\n",
    "    labels = batch['labels']\n",
    "    print(f'Texts: {texts}\\nLabels: {labels}\\n')\n",
    "    break\n",
    "\n",
    "# or build an iterator\n",
    "test_iter = iter(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dbdd4d4a-1590-4f0e-aa57-0223b097db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Briefly: Verizon opens global phone to consumers roundup Plus: IBM, Honda team on voice-driven car navigation...Linux seller completes name change...SAP names new VP...Amazon opens floor to political pundits.'] tensor([3])\n"
     ]
    }
   ],
   "source": [
    "example = next(test_iter)\n",
    "print(example['text'], example['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a0778-4839-4bd9-a2cc-8886bb1193a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
